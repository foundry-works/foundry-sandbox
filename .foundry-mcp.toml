# foundry-mcp configuration
#
# Configuration Priority (highest to lowest):
# 1. Environment variables (runtime overrides)
# 2. Project config (./foundry-mcp.toml)
# 3. User config (~/.foundry-mcp.toml)
# 4. Built-in defaults

# =============================================================================
# Workspace Configuration
# =============================================================================

[workspace]
# Where your specs live (auto-detected if not set).
# Good default: keep specs in ./specs.
specs_dir = "./specs"

# Notes inbox (defaults to specs_dir/.notes)
# Env var: FOUNDRY_MCP_NOTES_DIR
notes_dir = "./specs/.notes"

# Directory for storing research artifacts
research_dir = "./specs/.research"

# =============================================================================
# Logging Configuration
# =============================================================================

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# JSON logs are easier to ingest; set false for human-readable logs.
structured = true

# =============================================================================
# Tools Configuration
# =============================================================================

[tools]
# Disable specific tools to reduce prompt size.
# Tool descriptions are loaded into the model's context on each message.
# Good default: disable setup-only tools you do not use day-to-day.
#
# Available tools:
#   health       - Liveness/readiness checks (used by setup)
#   plan         - Plan creation and review workflows
#   pr           - Pull request creation
#   error        - Error collection and querying
#   journal      - Implementation journals
#   authoring    - Spec authoring operations
#   review       - Fidelity and code reviews
#   spec         - Spec management
#   task         - Task management
#   provider     - AI provider status
#   environment  - Environment setup and detection
#   lifecycle    - Spec lifecycle transitions
#   verification - Verification workflows
#   server       - Server introspection
#   test         - Test runner integration
#   research     - Research workflows (chat, consensus, thinkdeep, ideate, deep)
#
# Default: disable tools not needed, or only needed during setup
disabled_tools = ["health", "error"]

# Environment variable alternative: FOUNDRY_MCP_DISABLED_TOOLS (comma-separated)
# Example: FOUNDRY_MCP_DISABLED_TOOLS=error,research

# =============================================================================
# Observability Configuration
# =============================================================================
#
# Requires optional dependencies:
#   - For OpenTelemetry: pip install foundry-mcp[tracing]
#   - For Prometheus: pip install foundry-mcp[metrics]
#   - For both: pip install foundry-mcp[observability]

[observability]
# Master switch for all observability features
# Set to true to enable, then configure individual providers below
enabled = false

# =============================================================================
# Health Checks Configuration
# =============================================================================

[health]
# Health probes for liveness/readiness.
enabled = false

# =============================================================================
# Error Collection Configuration
# =============================================================================
#
# Stores error logs for observability and debugging.
# Errors are stored in append-only JSONL format with automatic cleanup.

[error_collection]
# Enable error collection
enabled = true

# =============================================================================
# Metrics Persistence Configuration
# =============================================================================
#
# Persist time-series metrics to disk so they survive restarts.

[metrics_persistence]
# Enable metrics persistence (default: false)
enabled = false

# =============================================================================
# Implement Command Configuration
# =============================================================================
#
# Default flags for the /implement command. These can be overridden via CLI flags.

[implement]

# Use subagent(s) for implementation (on by default)
delegate = true

# Run subagents concurrently for independent tasks (implies delegate=true)
parallel = true

# =============================================================================
# Git Workflow Configuration
# =============================================================================

[git]
# Enable git-aware workflows (automatic commit prompts, commit cadence, etc.)
enabled = true

# Determine when to offer automatic commits: "manual", "task", or "phase"
commit_cadence = "phase"

# Control automated behaviors
auto_commit = true
auto_push = true
auto_pr = false

# Show staged file preview before committing (recommended)
show_before_commit = false

# =============================================================================
# Workflow Configuration
# =============================================================================

[workflow]
# Execution mode:
#   "single"     - One task at a time with user approval
#   "autonomous" - Complete all phase tasks automatically
#   "batch"      - Execute batch_size tasks, then pause
mode = "autonomous"

# Automatically run validation after task completion
auto_validate = true

# Enable journaling of task completions
journal_enabled = true

# Number of tasks to execute in batch mode
batch_size = 5

# Context usage threshold (%) to trigger automatic pause
# When context reaches this threshold, autonomous/batch mode pauses
context_threshold = 85

# =============================================================================
# AI Consultation Configuration
# =============================================================================

[consultation]
# Default timeout for AI provider calls in seconds
default_timeout = 600

# Number of retry attempts per provider for transient failures
max_retries = 2

# Delay between retry attempts in seconds
retry_delay = 5.0

# Enable fallback to next available provider when one fails
fallback_enabled = true

# Cache time-to-live in seconds for consultation results
cache_ttl = 3600

# Provider priority list - first available provider wins
# Update order to match your preferred providers/models.
priority = [
    "[cli]codex:gpt-5.2-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.2-codex",
    "[cli]gemini:pro",
    "[cli]claude:opus",
]

# Per-provider overrides (optional)
# [consultation.overrides]
# "[cli]opencode:openai/gpt-5.2-codex" = { timeout = 600 }

# Per-workflow overrides
[consultation.workflows.fidelity_review]
# Good default: 2 models for consensus
min_models = 2
timeout_override = 600.0
default_review_type = "full"

[consultation.workflows.plan_review]
# Good default: 2 models for consensus
min_models = 2
default_review_type = "full"

[consultation.workflows.markdown_plan_review]
# Good default: 2 models
min_models = 2
timeout_override = 600.0
default_review_type = "full"

# =============================================================================
# Research Workflow Configuration
# =============================================================================

[research]
# Enable research tools (chat, consensus, thinkdeep, ideate, deep-research)
enabled = true

# Default LLM provider for research workflows
# Supports ProviderSpec format: "[cli]gemini:pro" or simple: "gemini"
default_provider = "[cli]gemini:pro"

# Providers for CONSENSUS workflow (multi-model consultation)
# Use the providers you have installed.
consensus_providers = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-5.2-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.2-codex",
    "[cli]claude:opus"
]

# State TTL in hours before cleanup
ttl_hours = 24

# Maximum messages per conversation thread
max_messages_per_thread = 100

# Default timeout for provider calls in seconds
# Minimum recommended: 600s for AI CLI providers
default_timeout = 600.0

# Maximum investigation depth for THINKDEEP workflow
thinkdeep_max_depth = 5

# Perspectives for IDEATE brainstorming
ideate_perspectives = ["technical", "creative", "practical", "visionary"]

# -----------------------------------------------------------------------------
# Deep Research Settings
# -----------------------------------------------------------------------------

# Maximum refinement iterations
deep_research_max_iterations = 3

# Maximum sub-queries per decomposition
deep_research_max_sub_queries = 5

# Maximum sources per sub-query
deep_research_max_sources = 10

# Follow and extract content from URLs
deep_research_follow_links = true

# Whole workflow timeout in seconds (recommended: 600s)
deep_research_timeout = 600.0

# Maximum parallel operations
deep_research_max_concurrent = 3

# Write audit artifacts for debugging
deep_research_audit_artifacts = true

# Research mode: controls source prioritization
# - "general"   : No domain preferences (default)
# - "academic"  : Prioritizes journals, publishers, preprints
# - "technical" : Prioritizes official docs, arxiv, Stack Overflow
deep_research_mode = "technical"

# Search providers (in priority order)
# Available: tavily, perplexity, google, semantic_scholar
deep_research_providers = [
    "semantic_scholar"
]

# -----------------------------------------------------------------------------
# Per-Phase Timeouts (override deep_research_timeout)
# Minimum recommended: 600s per operation for AI CLI providers
# -----------------------------------------------------------------------------

deep_research_planning_timeout = 600.0    # Query decomposition
deep_research_analysis_timeout = 600.0    # Finding extraction
deep_research_synthesis_timeout = 600.0   # Report generation (may take longer)
deep_research_refinement_timeout = 600.0  # Gap identification

# -----------------------------------------------------------------------------
# Per-Phase Providers (override default_provider)
# -----------------------------------------------------------------------------
# Supports ProviderSpec format for model selection:
#   "[cli]gemini:pro"
#   "[cli]claude:opus"
#   "[cli]opencode:openai/gpt-5.2-codex"
#   "[cli]codex:gpt-5.2-codex"
#   "[cli]cursor-agent:gpt-5.2-codex"

deep_research_planning_provider = "[cli]gemini:flash"
deep_research_analysis_provider = "[cli]gemini:pro"
deep_research_synthesis_provider = "[cli]gemini:pro"
deep_research_refinement_provider = "[cli]gemini:pro"

# -----------------------------------------------------------------------------
# Per-Phase Fallback Provider Lists (Retry & Resilience)
# -----------------------------------------------------------------------------
# Each phase can have an ordered list of fallback providers.
# On failure/timeout, the workflow retries with backoff, then tries
# the next provider in the list until success or exhaustion.
# Empty list = no fallback (use only the primary provider)

# Planning phase: query decomposition (can use faster/cheaper models)
deep_research_planning_providers = [
    "[cli]gemini:flash",
    "[cli]codex:gpt-5.1-codex-mini",
    "[cli]opencode:openai/gpt-5.1-codex-mini",
    "[cli]cursor-agent:gpt-5.2-codex-fast",
    "[cli]claude:sonnet"
]

# Analysis phase: finding extraction
deep_research_analysis_providers = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-4.1",
    "[cli]opencode:openai/gpt-4.1",
    "[cli]cursor-agent:gpt-4.1",
    "[cli]claude:opus"
]

# Synthesis phase: report generation (may benefit from stronger models)
deep_research_synthesis_providers = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-5.2-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.2-codex",
    "[cli]claude:opus"
]

# Refinement phase: gap identification
deep_research_refinement_providers = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-5.2-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.2-codex",
    "[cli]claude:opus"
]

deep_research_max_retries = 2
deep_research_retry_delay = 5.0

# -----------------------------------------------------------------------------
# Search Rate Limiting
# -----------------------------------------------------------------------------

search_rate_limit = 60              # Requests per minute (global)
max_concurrent_searches = 3         # Concurrent search requests

[research.per_provider_rate_limits]
tavily = 60
perplexity = 60
semantic_scholar = 100

# =============================================================================
# Test Runner Configuration
# =============================================================================
#
# Configure which test runner to use and customize runner settings.
# The foundry-setup command can auto-detect and configure this section.
#
# Supported runners (built-in defaults): pytest, go, npm, jest, make
# Custom runners can be defined in [test.runners.*] sections.

[test]
# Default runner to use when running tests
# Valid values: pytest, go, npm, jest, make, or any custom runner name
default_runner = "pytest"

# Example custom runner:
# [test.runners.go]
# command = ["go", "test"]
# run_args = ["./..."]
# pattern = "*_test.go"
# timeout = 300
